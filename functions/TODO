+ Change elevation grid loading: don't duplicate DEM, rather (for both DEM and DHM) make a List of Lists: (for dhm) 

list(elevation = list(grids),
     elevation_corr = list(grids),  # Elevation grid processed with hydrological flow correction, to compute avalanches
     slope_corr = list(grids),
     aspect_corr = list(grids),
     deposition_max = list(grids),
     draining_fraction = list(draining_fraction1 = list(grids), draining_fraction2 = list(grids), draining_fraction3 = list(grids), draining_fraction4 = list(grids)),
     grid_year_id = rep(NA, run_params$n_years)),
    
so that we keep track of which DEM belongs to which year. Unless we interpolate the DEM (in which case we will have one grid per year), this saves a lot of memory and some time. Compute slope and aspect when you load the DEM and DHM, saves time later.

+ Move computation of avalanche-related fixed grids (lines 24-140 of func_snow_transport.R) into own function, to be called while loading DHMs to fill the data_dhms list (called from function func_load_elevation_grids when parameter "dhm" is passed).

+ Add switch to load an RData file with all the data, rather than loading and checking everything every time. And another one to save all the data, to be later loaded.



+ ask Matthias:
    - Reduction of small scale variability (red_factor=0.5): what is the idea behind this reduction? Any tip on setting this reduction value?
    - About the optimization loops: it looks like the entire annual workflow is performed also when doing just the winter optimization? Could it become more efficient to just simulate the winter period when doing the winter optimization?
    - About avalanches: I have implemented Gruber (2007), fast and well performing. This simulates avalanche events, i.e. actual downstream mass transport. Any tips on deploying it in the simulation? (1) fixed date single avalanche, (2) recurrent avalanche in winter (e.g. monthly), (3) avalanche every time the max daily/weekly snowfall over the glacier exceeds a certain amount)

+ make compute_snowdist_topographic similar to avalanche_fixed_grids (work on all grids, compute everything and put into a list of rasters)

+ func_select_year_measurements(data_massbal, logical annual_or_winter): from a mass balance data.frame (annual or winter) return the indices of all the lines where the measurement period ENDS in the current year (before 1 Oct).
  
+ func_compute_initial_snow_cover(run_params, year, data_dhms, data_avalanche, data_dems, grids_snowdist_topographic, data_probes):
    # Compute avalanche on the topographic snow dist (with appropriate multiplier for max deposition)
    # Reduce effect of the computed grid (custom reduction parameter, in IDL it is 0.5) and normalize the resulting grid over the glacier surface
    # Compute probes_idw, normalize the probes idw output
    # Multiply the two grids (probes idw and avalanched topographic dist) and normalize again over the glacier surface
    # Compute grid with snowgrad and snow line elevation
    # Combine with the distribution grid and return
    
+ continue main loop, with:
  + # determination of which mass balance measurements we use this year (both annual and winter): call func_select_year_measurements()
  + # creation of initial snow cover (estimate with func_compute_initial_snow_cover, or use result from previous modeling)
  + determination of year boundaries for modeling (should cover measurement period and hydrological year)
  + model run over the year with initial parameters
    
    
+ for the model run:
    + What about avalanches? (1) fixed date single avalanche, (2) recurrent avalanche in winter (e.g. monthly), (3) avalanche every time the max snowfall over the glacier exceeds a certain amount)
    + BEFORE (just once per year): find the 4 cells surrounding each stake, using fourCellsFromXY; we'll use them to compute model misfit.
    + Model run: simulation (year or winter) <- func_massbal_model(run_params, cur_model_params, model_year_boundaries); simulation is a list with numeric arrays of the simulation results, storing each day.
        + setup variables: numeric vectors, length nrow*ncol*ndays, storing each daily map in nrow*ncol components of the vector.
        *** Vectors: surf_type_cur, snow_swe, cumul_massbal. At the beginning surf_type_cur is the surf_type grid (from file) plus the initial snow cover.
        + loop
            + compute avalanche_cond (e.g. check if date == fixed date for avalanche, or check an accumulation variable against a threshold value)
            + if avalanche_cond == TRUE {
                + do avalanche over the snow swe grid, with a smart choice of the movable fraction (snow age? Or use snow swe - initial snow, or pmax(0, snow swe - last_snow_checkpoint))
                + re-compute surface types
                + if needed, reset the values leading to avalanche_cond = TRUE (e.g. accumulated snow; not needed for fixed dates)}
            + do melt model (select cells with exposed ice and melt them, select cells with exposed firn and melt them, select cells with exposed debris and melt them, select cells with exposed snow and melt them).
            + do accumulation (add snowfall to snow_swe)
            + update the surface type (where snow_swe > 0 set snow, else set firn/ice depending on the original (file) grid)
    
+ Run avalanche in the annual simulation

+ In the model loop, compute daily glacier-wide mass balance and store within an array, to be returned together with the other vectors (cumulative mass balance, and swe).

+ Implement optimization of BIAS and RMS (together? such as minimizing (abs(BIAS) + 1) * RMS) (Minimizing abs(BIAS) * RMS would stop at BIAS = 0, independently of RMS; another option: abs(BIAS) + RMS). main2.R has the multi-threaded version of the optimization.

+ Make data_surftype follow the same scheme as the elevation grids (i.e. each grid is loaded only once, and referenced with grid_id).

+ Finish xyzn loading, with same scheme as grids.

+ Function to do a single run, either annual or winter, with first argument a numeric vector of named multipliers, return value a list with modeled mass balance, stakes series and stakes bias and rms.
This function is called by an optimization function which looks at the stakes bias and rms.

+ Implement variable ice albedo (TELL MATTHIAS HIS IMPLEMENTATION IS WRONG, it has no "exponential increase of r_ice", rather it is linear with elevation difference from a cutoff elevation).

- Discuss with Matthias about summer avalanches: currently we have implemented multiple fixed-date events, we can also use a simple condition such as (daily_precip > X and temperature < Y; X and Y custom parameters).

+ Implement winter optimization; optimizations: in winter cprec, in summer melt_factor and rad_fact_ice.

+ Implement correction in elevation bands for the final model (over the measurement period).

+ Implement saving of previous year's snow map, used as starting condition.

+ Implement reading of legacy param files, for easier integration with IDL input.

- Implement possibility to NOT run optimization (i.e., only elevation bands correction). Put it in the func_optimize_mb_annual(): a switch, so that we don't run uniroot() if the switch is on (we just run once the model). Rename this function to func_run_annual() to accommodate the new use.

+ Computation of ELA and AAR. For the ELA: divide the output (corrected) mass balance maps in 10 m elevation bands, compute the mean mass balance for each band, and take the band with index which.min(abs(band mass balance)). The ELA is undefined if the mass balance of such a band is not close to 0 (entire glacier snow-covered or snow-free), and it is not well defined if there are at least two bands at least 0.2*glacier_ele_span apart and both with mass balance close to 0 (e.g. for avalanches). NOTE: in principle one should do this for every day (i.e. computing the TSL) and then select the highest TSL, which is the ELA. In practice, the IDL version just looks at the massbalfix. Looking at each day (with the loop over elevation bands) would be quite slow. So we just use the corrected mass balance too.

+ Output: implement overview PDF (time series of corrected annual mass balance, time series of (hydro, measurement and fixed) annual mass balance, time series of winter mass balance (measurement if available, and fixed), ELA, AAR, RMSE, melt parameters, prec_corr, cumulative mass balance.

+ Implement stakes clustering: after loading the mass balance data (still before the main loop), compute their spatial distance matrix, then also two distance matrices of the start and end dates; for NA starting dates, check the year of the end dates, and consider as same date all stakes with NA start and same end year (easiest: compute new column "start_date_temp" which is equal to the starting date, then replace the NAs with 1 Jan of the year before the year of stake end). From the two matrices of temporal distance compute 1/((dist_datestart == 0) * (dist_dateend == 0)): Inf if start and/or end are different, 1 if they are the same. Finally compute clustering with a parameter run_params$stake_cluster_distance (https://gis.stackexchange.com/questions/64392/finding-clusters-of-points-based-distance-rule-using-r) over the product of the spatial distance and this time distance.

+ Implement computation of corrected glacier-wide mass balance series.

+ Implement writing of plot data: run_params$grid_out_format (either "geotiff" or "ascii"), to output the rasters; also, text files with data tables of mass balance time series (at the glacier and at the stakes), and df_overview.dat with everything which is shown in the overview plots.

+ On the plots including stake measurements, add [option, or automatic always-on feature] to "correct" the stakes to the full measurement period, i.e. add to the stake value the modeled MB change between (start of the measurement period - stake start) and between (stake end - end of the measurement period). This way the stake values are actually comparable to the mass balance modeled over the single "measurement period" (both "raw" and corrected in elevation bands).
Compute this corrected value as an additional column in the massbal_annual_meas_cur data frame. It needs to be shown in the following plots: (1) mass balance map (annual, corrected) with stakes; (2) mass balance map (annual, not corrected) with stakes (plot currently not generated, but might be interesting); (3) scatterplot of mass balance vs altitude

+ Investigate why the model stake bias is different for stake 16 of year_id = 1, if computed on the stake period itself vs on the measurement period (with the standardized stake value which is corrected by using the model, so there should NOT be a change in the bias!!) SOLUTION: it came from our manual bilinear filtering in case of aligned stakes-cell centers. Fixed adding duplicates = FALSE in fourCellsFromXY() while changing the definition of dy1 and dy2 (so that dy2 can be 0 and dy1 cannot), since dy2 refers to the two cells on the row above the point and duplicates = FALSE will usually add only cells on the row below.
Commands for this:
> (extract(massbal_annual_maps$meas_period, cbind(massbal_annual_meas_cur$x, massbal_annual_meas_cur$y), method = "bilinear") - massbal_annual_meas_cur$massbal_standardized) # Bias over the measurement period
> (mod_output_annual_cur$stakes_mb_mod - mod_output_annual_cur$stakes_mb_meas) # Bias over the stake period.
> (extract(massbal_annual_maps$meas_period, cbind(massbal_annual_meas_cur$x, massbal_annual_meas_cur$y), method = "bilinear") - massbal_annual_meas_cur$massbal_standardized) - (mod_output_annual_cur$stakes_mb_mod - mod_output_annual_cur$stakes_mb_meas)

- Output: plots of the stake residuals against total annual radiation, to help the user set the model parameters?

- Implement lines 1002-1012 (account for winter misfit in annual optimization) (not used by Martina so far, maybe later? We don't have winter data in Central Asia anyway)

+ Move most of the main loop to functions!

+ Implement loading of outline as shapefile (check extension of the outline name suffix)



---- BELOW: nice to have but not needed, or too messy to implement, or not really working ----

[[- Make func_compute_initial_snow_cover use single-year grids (selected in the main loop) rather than selecting the grid inside the function (we need single grids for both func_compute_initial_snow_cover and func_massbal_model, so we don't extract them twice).]]: no real advantage of doing this; both functions now take the whole data_dems etc. and select their single grids from inside. Selecting in the main loop would make it even longer.

[[- New form for optimization:
    - for the winter optimization, optimize variables cprec and precgrad; first do cprec with precgrad fixed, until bias is 0; then do 2 runs to compute the local derivative (try cprec_zero_bias + dx, and then precgrad + dy); then alter together cprec and precgrad inversely, so that the bias stays close to 0, and try to reduce rms (use a parallel 4-nodes cluster: first run with a slight and a moderate change in the two directions, check that the moderate change actually worsens the RMS (if one of the two extremes has the lowest RMS, extend more in the two directions!), then pick the interval whose endpoints have the lowest average RMS (between the two), and cut the interval with 4 uniform sampling points (i.e. 5 sub-intervals), again take the sub-interval with the best endpoints and continue. Stop whenever none of the 4 children improves on the best of the 2 parents. At the end, check for any residual bias (which could arise due to the non-constant local derivative, i.e. non-linear roles of cprec and precgrad away from the first zero_bias point), and remove it with cprec (one run to determine derivative, another to cancel the bias).
    - for the summer optimization, do the same but for variables melt_factor and rad_fact_ice. Keep constant the ratio of rad_fact_ice and rad_fact_snow. So: first scale melt_factor and rad_fact_ice by the same factor until bias is 0, then move them in opposite directions (according to local derivative!) trying to minimize rms.]]: APPARENTLY RMS (at Barkrak) DOES NOT IMPROVE WHEN PLAYING WITH THE PARAMETERS!!

[[- Put transport_deposit_mass inside a minimal package, so that it can be loaded instead of doing sourceCpp.]]: NOT FASTER THAN R version! (well vectorized, short loop)

[[- At the moment we model the whole rectangular grid (for avalanches). A possible speedup would be to compute the glacier's watershed at the beginning, then model only those cells, so that we consider only all the cells whose avalanches may interact with the glacier. To compute the watershed: topmodel::subcatch(ele, c(outlet_row, outlet_col)), we compute the watershed of the lowest glacier cell, then while there are glacier cells outside the computed catchment we add to it the catchment of the lowest glacier cell which is outside the catchment computed so far, and we continue until the whole glacier is included. This also accommodates glaciers with more than one outlet. Then we have a list of cells which we care about.]]: this is at high risk of messing up the avalanche routine real bad.

